{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ddb2ae-9d5b-44dc-9b8b-f8d962363b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, LeakyReLU \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "\n",
    "# to display confusion matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1004df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Avasilable:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:52:48.376184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 14:52:48.393759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 14:52:48.393882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "#GPU memory growth\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Avasilable: \", len(physical_devices))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a68d767-84ae-40ce-be4e-6aa75c756d31",
   "metadata": {},
   "source": [
    "### Aux function to show a batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e47a79-f0f0-4173-b453-80d19d0da3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(image_batch, label_batch, epoch):\n",
    "  columns = 5\n",
    "  rows = BATCH_SIZE / columns + 1  \n",
    "  epochImages = plt.figure(figsize=(15, 3 * rows))\n",
    "  epochImages.suptitle('epoch {}'.format(epoch))\n",
    "  for n in range(BATCH_SIZE):\n",
    "      ax = plt.subplot(int(rows), columns, n+1)\n",
    "      plt.imshow((image_batch[n]))\n",
    "      plt.title(classNames[label_batch[n]==1][0])\n",
    "      plt.axis('off')\n",
    "\n",
    "def show_batchSimple(image_batch, label_batch):\n",
    "  columns = 6\n",
    "  rows = BATCH_SIZE / columns + 1  \n",
    "  plt.figure(figsize=(10, 2 * rows))\n",
    "  for n in range(BATCH_SIZE):\n",
    "      ax = plt.subplot(int(rows), columns, n+1)\n",
    "      plt.imshow((image_batch[n]))\n",
    "      plt.title(classNames[label_batch[n]==1][0])\n",
    "      plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e6c1e-b693-4f85-aa92-982b79507674",
   "metadata": {},
   "source": [
    "### Auxiliary functions to load image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59708542-329a-4866-bb08-cc85edb7e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == classNames\n",
    "\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_png(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [WIDTH, HEIGHT])\n",
    "\n",
    "def get_bytes_and_label(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7cc8bc-3cc8-4ed3-8dd4-22597a846c7c",
   "metadata": {},
   "source": [
    "### Function to process images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e400b5f9-c851-4b10-9c17-ee3afaf0d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image, label):\n",
    "    \n",
    "    #image = tf.image.resize(image, (64,64))\n",
    "    \n",
    "    r = tf.random.uniform(shape=(), minval=0, maxval=0.5) - 0.25\n",
    "    image = tfa.image.rotate(image, r)\n",
    "    \n",
    "    rx = tf.random.uniform(shape=(), minval=0, maxval=20) - 10\n",
    "    ry = tf.random.uniform(shape=(), minval=0, maxval=8) - 4\n",
    "    image = tfa.image.translate(image, [rx, ry])\n",
    "    #image = tfa.image.random_hsv_in_yiq(image, 0.5, 0.9, 1.1, 0.9, 1.3)\n",
    "    #image = tf.image.resize(image, (32,32))\n",
    "    image = tf.clip_by_value(tfa.image.random_hsv_in_yiq(image, \n",
    "                    0.4,\n",
    "                    0.4,1.4,\n",
    "                    0.4, 1.4), 0.0, 1.0)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da479c",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae997b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    print(history.history.keys())\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='lower right')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.show()    \n",
    "    \n",
    "    \n",
    "def show_accuracies(): \n",
    "    fig, ax = plt.subplots()\n",
    "    X = np.arange(2)\n",
    "\n",
    "    models = ['bad val set', 'good val set']\n",
    "    plt.bar(X, [evalV1[1], evalV2[1]], width = 0.4, color = 'b', label='test')\n",
    "    plt.bar(X + 0.4, [valV1[1], valV2[1]], color = 'r', width = 0.4, label = \"val\")\n",
    "    plt.xticks(X + 0.4 / 2, models)\n",
    "    plt.ylim(top = 1.0, bottom = 0.80)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def show_misclassified(predictions, ground_truth, images, num_rows= 5, num_cols=3):\n",
    "    \n",
    "    # Plot the first X test images with wrong predictions.\n",
    "    num_images = num_rows*num_cols\n",
    "    plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "    i = 0\n",
    "    k = 0\n",
    "    while k < len(images) and i < num_images:\n",
    "        predicted_label = np.argmax(predictions[k])\n",
    "        gt = np.where(ground_truth[k])[0][0]\n",
    "        if predicted_label != gt:\n",
    "            plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "            plot_image(k, predictions[k], gt, images)\n",
    "            plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "            plot_value_array(k, predictions[k], ground_truth)\n",
    "            i += 1\n",
    "        k += 1\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "  predictions_array, true_label, img = predictions_array, true_label, img[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "\n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(classNames[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                classNames[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "  predictions_array, true_label = predictions_array, true_label[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks(range(43))\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(43), predictions_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1])\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[np.where(true_label)[0][0]].set_color('blue')    \n",
    "\n",
    "def plot_predictions(predictions, ground_truth, images, num_rows= 5, num_cols=3 ):\n",
    "\n",
    "    num_images = num_rows*num_cols\n",
    "    plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "    for i in range(min(num_images,len(images))):\n",
    "        gt = np.where(ground_truth[i])[0][0]\n",
    "        plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "        plot_image(i, predictions[i], gt, images)\n",
    "        plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "        plot_value_array(i, predictions[i], ground_truth)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f1c0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_callbacks(file_path):\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath= file_path, \n",
    "                               monitor = 'val_accuracy',\n",
    "                               verbose=1, \n",
    "                               save_weights_only=True,\n",
    "                               save_best_only=True)\n",
    "\n",
    "\n",
    "    earlyStopper = EarlyStopping(monitor='val_loss', min_delta = 0.0001, patience = 15, verbose = 1)\n",
    "\n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.000000001, verbose = 1)\n",
    "\n",
    "    return [checkpointer, earlyStopper, reduceLR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc271390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths para os datasets\n",
    "pathTrain = 'GTSRB_TP/train_images'\n",
    "pathTest = 'GTSRB_TP/test_images'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "HEIGHT = 32\n",
    "WIDTH = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd0e51-521d-44b7-b0c8-b7c43928484c",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "848c9f65-0ec6-4a76-9912-5d2c48c6112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00013' '00025' '00003' '00028' '00035' '00026' '00036' '00027' '00001'\n",
      " '00019' '00020' '00042' '00002' '00038' '00021' '00024' '00034' '00029'\n",
      " '00033' '00006' '00041' '00017' '00030' '00009' '00040' '00000' '00004'\n",
      " '00015' '00008' '00012' '00016' '00037' '00022' '00039' '00010' '00031'\n",
      " '00005' '00011' '00023' '00018' '00014' '00032' '00007']\n"
     ]
    }
   ],
   "source": [
    "data_dir = pathlib.Path(pathTrain)\n",
    "classNames = np.array(os.listdir(data_dir))\n",
    "print(classNames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "580dc4b2-3641-4409-a473-974c1d97575f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in dataset:  39220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:52:48.725768: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-01 14:52:48.726234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 14:52:48.726376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 14:52:48.726439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 14:52:48.985434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 14:52:48.985571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 14:52:48.985679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 14:52:48.985751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5004 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "listset = tf.data.Dataset.list_files(pathTrain+\"/*/*.png\")\n",
    "dataset = listset.map(get_bytes_and_label, num_parallel_calls = AUTOTUNE)\n",
    "\n",
    "train_Data_length = tf.data.experimental.cardinality(dataset).numpy()\n",
    "print(\"Total images in dataset: \", train_Data_length)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc3daa28-6443-4e48-a863-d8874e4459c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_dataset = dataset\n",
    "dataset = dataset.shuffle(buffer_size = train_Data_length)\n",
    "#train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "#train_dataset = train_dataset.map(process_image)\n",
    "# shuffle elements, to shuffle batches place shuffle after batch\n",
    "#train_dataset = train_dataset.batch(batch_size = BATCH_SIZE)\n",
    "#train_dataset = train_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69c976ee-5175-4638-a70a-8de942122ce5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/falape/Projetos/VCPI_GTSRB/gtsrb_Static_augmentation.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/falape/Projetos/VCPI_GTSRB/gtsrb_Static_augmentation.ipynb#ch0000017?line=0'>1</a>\u001b[0m image_batch, label_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(dataset))        \n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/falape/Projetos/VCPI_GTSRB/gtsrb_Static_augmentation.ipynb#ch0000017?line=1'>2</a>\u001b[0m show_batch(image_batch, label_batch\u001b[39m.\u001b[39;49mnumpy(), \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/falape/Projetos/VCPI_GTSRB/gtsrb_Static_augmentation.ipynb#ch0000017?line=3'>4</a>\u001b[0m image_batch, label_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(dataset))      \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/falape/Projetos/VCPI_GTSRB/gtsrb_Static_augmentation.ipynb#ch0000017?line=4'>5</a>\u001b[0m show_batch(image_batch, label_batch\u001b[39m.\u001b[39mnumpy(), \u001b[39m2\u001b[39m)\n",
      "\u001b[1;32m/home/falape/Projetos/VCPI_GTSRB/gtsrb_Static_augmentation.ipynb Cell 4'\u001b[0m in \u001b[0;36mshow_batch\u001b[0;34m(image_batch, label_batch, epoch)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/falape/Projetos/VCPI_GTSRB/gtsrb_Static_augmentation.ipynb#ch0000003?line=6'>7</a>\u001b[0m ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplot(\u001b[39mint\u001b[39m(rows), columns, n\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/falape/Projetos/VCPI_GTSRB/gtsrb_Static_augmentation.ipynb#ch0000003?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow((image_batch[n]))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/falape/Projetos/VCPI_GTSRB/gtsrb_Static_augmentation.ipynb#ch0000003?line=8'>9</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(classNames[label_batch[n]\u001b[39m==\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/falape/Projetos/VCPI_GTSRB/gtsrb_Static_augmentation.ipynb#ch0000003?line=9'>10</a>\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x2980.8 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#não está funcionar porque são faço batches no dataset\n",
    "# image_batch, label_batch = next(iter(dataset))        \n",
    "# show_batch(image_batch, label_batch.numpy(), 1)\n",
    "\n",
    "# image_batch, label_batch = next(iter(dataset))      \n",
    "# show_batch(image_batch, label_batch.numpy(), 2)\n",
    "\n",
    "# image_batch, label_batch = next(iter(dataset))      \n",
    "# show_batch(image_batch, label_batch.numpy(), 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6919c0d2-e1f0-4ee3-9587-312955ed370a",
   "metadata": {},
   "source": [
    "### Define simpler functions for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc55ca-96c7-49d4-87a3-17ad569b2454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_trans(image, label):\n",
    "    \n",
    "    rx = tf.random.uniform(shape=(), minval=0, maxval=20) - 10\n",
    "    ry = tf.random.uniform(shape=(), minval=0, maxval=8) - 4\n",
    "    image = tfa.image.translate(image, [rx, ry])\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def process_image_rot(image, label):\n",
    "    \n",
    "    r = tf.random.uniform(shape=(), minval=0, maxval=0.5, dtype=tf.dtypes.float32) - 0.25\n",
    "    image = tfa.image.rotate(image, r)\n",
    "    image = tf.clip_by_value(tfa.image.random_hsv_in_yiq(image, 0.0, 0.4, 1.1, 0.4, 1.1), 0.0, 1.0)\n",
    "\n",
    "    image = tf.clip_by_value(tf.image.adjust_brightness(image, tf.random.uniform(shape=(), minval=0, maxval=0.1)-0.2),0,1)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d1c16-353f-4a54-a687-51d787dc5e00",
   "metadata": {},
   "source": [
    "## Create a dataset with twice the number of images, one subset is translated, the other is rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c9b8f-6600-4e98-911b-12a5bb9d93cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasetA = dataset\n",
    " \n",
    "train_dataset = datasetA.map(process_image_trans)\n",
    "train_dataset = train_dataset.concatenate(datasetA.map(process_image_rot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98961604",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_length = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "print(\"Total images in dataset: \", train_dataset_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(buffer_size = train_dataset_length)\n",
    "train_dataset = train_dataset.batch(batch_size = BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(buffer_size = AUTOTUNE)\n",
    "train_dataset = train_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac2d06-203d-4b18-85de-a7837ffe3b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))        \n",
    "show_batch(image_batch, label_batch.numpy(), 1)\n",
    "image_batch, label_batch = next(iter(train_dataset))      \n",
    "show_batch(image_batch, label_batch.numpy(), 2)\n",
    "image_batch, label_batch = next(iter(train_dataset))      \n",
    "show_batch(image_batch, label_batch.numpy(), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a955b0ad-0a19-4985-9967-185fc17e2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_size = int(0.8*train_dataset_length)\n",
    "val_size = int(0.2*train_dataset_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa2705",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Data = train_dataset.take(train_size)\n",
    "val_Data = train_dataset.skip(train_size)\n",
    "print(tf.data.experimental.cardinality(train_Data).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff2537",
   "metadata": {},
   "source": [
    "### Test Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc08d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(pathTest)\n",
    "classNames = np.array(os.listdir(data_dir))\n",
    "print(classNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ce010",
   "metadata": {},
   "outputs": [],
   "source": [
    "listset = tf.data.Dataset.list_files(pathTest+\"/*/*.png\")\n",
    "test_Data = listset.map(get_bytes_and_label, num_parallel_calls = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813fa4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Data_length = tf.data.experimental.cardinality(test_Data).numpy()\n",
    "print(\"Total images in dataset: \", test_Data_length)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Data = test_Data.batch(batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(test_Data))        \n",
    "show_batchSimple(image_batch, label_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32676d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(128, (5, 5),\n",
    "                    input_shape=(32, 32, 3)))         \n",
    "model.add(LeakyReLU(alpha=0.01))  \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5)) \n",
    "\n",
    "model.add(Conv2D(196, (5, 5) )) \n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5)) \n",
    "\n",
    "model.add(Conv2D(256, (5, 5) ) )   \n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5)) \n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(LeakyReLU(alpha=0.0)) \n",
    "model.add(Dense(384))\n",
    "model.add(LeakyReLU(alpha=0.0))             \n",
    "model.add(Dropout(0.5)) \n",
    "\n",
    "model.add(Dense(43, activation='softmax'))\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(optimizer = opt, loss='categorical_crossentropy', metrics=[ 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048420f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './TrainModels/static_Augmentation.ckpt'\n",
    "\n",
    "callbacks = prepare_callbacks(file_path)\n",
    "\n",
    "history = model.fit(train_Data, steps_per_epoch = train_size/BATCH_SIZE,\n",
    "          epochs=30, \n",
    "          validation_data=val_Data,\n",
    "          validation_steps=val_size/BATCH_SIZE, \n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d806f6",
   "metadata": {},
   "source": [
    "### Load Modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4cf276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(file_path)\n",
    "\n",
    "#evalV1 = model.evaluate(test_Data, verbose=2)\n",
    "#valV1 = model.evaluate(val_Data, steps=1, batch_size=0.2 * train_Data_length, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0fefae",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = model.evaluate(test_Data, verbose=2)\n",
    "print(model_eval)\n",
    "\n",
    "model_val = model.evaluate(val_Data, steps=1, batch_size=val_size, verbose=2)\n",
    "print(model_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b5ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_labels = []\n",
    "numpy_images = []\n",
    "pred = []\n",
    "\n",
    "for images, labels in test_Data.take(-1):  # take all batches of dataset\n",
    "    numpy_images.extend(images.numpy())\n",
    "    numpy_labels.extend(labels.numpy())\n",
    "    pred.extend(model.predict(images.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30592897",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_misclassified(pred, numpy_labels, numpy_images, int((val_size - val_size*.9861 )/3 + 1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
