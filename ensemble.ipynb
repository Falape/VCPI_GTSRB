{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, LeakyReLU, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "\n",
    "# to display confusion matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "# to determine the most voted\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == classNames\n",
    "\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_png(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [32,32])\n",
    "\n",
    "def get_bytes_and_label(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label\n",
    "\n",
    "def show_data(s1,l1, s2,l2, labels, min):\n",
    "    fig, ax = plt.subplots()\n",
    "    X = np.arange(len(s1))\n",
    "\n",
    "    models = labels\n",
    "    plt.bar(X, s1, width = 0.4, color = 'b', label=l1)\n",
    "    plt.bar(X + 0.4, s2, color = 'r', width = 0.4, label = l2)\n",
    "    plt.xticks(X + 0.4 / 2, models)\n",
    "    plt.ylim(top = 100, bottom = min)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def show_batch(image_batch, label_batch):\n",
    "  columns = 6\n",
    "  rows = BATCH_SIZE / columns + 1  \n",
    "  plt.figure(figsize=(10, 2 * rows))\n",
    "  for n in range(BATCH_SIZE):\n",
    "      ax = plt.subplot(int(rows), columns, n+1)\n",
    "      plt.imshow((image_batch[n]))\n",
    "      plt.title(classNames[label_batch[n]==1][0])\n",
    "      plt.axis('off')\n",
    "\n",
    "\n",
    "def show_history(history):\n",
    "    print(history.history.keys())\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='lower right')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "def show_accuracies(labels, test, val): \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    X = np.arange(len(test))\n",
    "\n",
    "    plt.bar(X, test, width = 0.4, color = 'b', label='test')\n",
    "    plt.bar(X + 0.4, val, color = 'r', width = 0.4, label = \"val\")\n",
    "    plt.xticks(X + 0.4 / 2, labels)\n",
    "    plt.ylim(top = 1.0, bottom = 0.97)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "\n",
    "def show_misclassified(predictions, ground_truth, images, num_rows = 5, num_cols=3):\n",
    "    \n",
    "    # Plot the first X test images with wrong predictions.\n",
    "    num_images = num_rows*num_cols\n",
    "    print(num_images)\n",
    "    plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "    i = 0\n",
    "    k = 0\n",
    "    while k < len(images) and i < num_images:\n",
    "        predicted_label = np.argmax(predictions[k])\n",
    "        gt = np.where(ground_truth[k])[0][0]\n",
    "        if predicted_label != gt:\n",
    "            plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "            plot_image(k, predictions[k], gt, images)\n",
    "            plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "            plot_value_array(k, predictions[k], ground_truth)\n",
    "            i += 1\n",
    "        k += 1\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    predictions_array, true_label, img = predictions_array, true_label, img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    \n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "      color = 'blue'\n",
    "    else:\n",
    "      color = 'red'\n",
    "    \n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(classNames[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                classNames[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array, true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(8))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(8), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[np.where(true_label)[0][0]].set_color('blue')   \n",
    "\n",
    "\n",
    "\n",
    "def show_confusion_matrix(mat, classes):\n",
    "\n",
    "    df_cm = pd.DataFrame(mat, range(classes), range(classes))\n",
    "    plt.figure(figsize=(15,10))\n",
    "    sn.set(font_scale=1.4) # for label size\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='d') # font size\n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_callbacks(file_path):\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath= file_path, \n",
    "                               monitor = 'val_accuracy',\n",
    "                               verbose=1, \n",
    "                               save_weights_only=True,\n",
    "                               save_best_only=True)\n",
    "\n",
    "\n",
    "    earlyStopper = EarlyStopping(monitor='val_loss', min_delta = 0.0001, patience = 15, verbose = 1)\n",
    "\n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.000000001, verbose = 1)\n",
    "\n",
    "    return [checkpointer, earlyStopper, reduceLR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 32\n",
    "\n",
    "DATA_PREFIX = 'GTSRB_TP/'\n",
    "\n",
    "TRAIN_ONLINE = False # if False it will train models, otherwise it will load weights\n",
    "trained_models = ['TrainModels/static_Augmentation.ckpt', 'TrainModels/static_Augmentation_balance.ckpt',\n",
    "'TrainModels/noAugmentation.ckpt','TrainModels/noAugmentation_Balance.ckpt', 'TrainModels/Dynamic_Augmentation.ckpt', 'Massive_Augmentation.ckpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(DATA_PREFIX + 'train_images/')\n",
    "  \n",
    "classNames = np.array(os.listdir(data_dir))\n",
    "NUM_CLASSES = len(classNames)\n",
    "classNames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "listset = tf.data.Dataset.list_files(DATA_PREFIX + 'train_images/*/*.png')\n",
    "dataset = listset.map(get_bytes_and_label, num_parallel_calls = AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = next(iter(dataset))\n",
    "print(t[0].shape, t[1].shape)\n",
    "\n",
    "# note: this only works if dataset is not repeating\n",
    "dataset_length = tf.data.experimental.cardinality(dataset).numpy()\n",
    "print(\"Total images in dataset: \", dataset_length)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataSolo = dataset\n",
    "dataSolo = dataSolo.cache()\n",
    "dataSolo = dataSolo.shuffle(buffer_size = dataset_length)\n",
    "dataSolo = dataSolo.prefetch(buffer_size=dataset_length)\n",
    "dataSolo = dataSolo.batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8*dataset_length)\n",
    "val_size = int(0.2*dataset_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSolo = dataSolo.take(train_size)\n",
    "valset = dataSolo.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_listset = tf.data.Dataset.list_files(DATA_PREFIX + 'test_images_per_folder/*/*.png')\n",
    "test_dataset_length = test_listset.cardinality().numpy()\n",
    "print(\"Total images in validatation dataset: \", test_dataset_length)\n",
    "\n",
    "testset = test_listset.map(get_bytes_and_label, num_parallel_calls = AUTOTUNE)\n",
    "testset = testset.batch(batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(dataSolo))        \n",
    "show_batch(image_batch, label_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(classCount, imgSize, channels):\n",
    "    modelLogits = Sequential()\n",
    "\n",
    "    modelLogits.add(Conv2D(128, (5, 5),\n",
    "                        input_shape=(32, 32, 3)))         \n",
    "    modelLogits.add(LeakyReLU(alpha=0.01))  \n",
    "    modelLogits.add(BatchNormalization())\n",
    "    modelLogits.add(Dropout(0.5)) \n",
    "\n",
    "    modelLogits.add(Conv2D(196, (5, 5) )) \n",
    "    modelLogits.add(LeakyReLU(alpha=0.01))\n",
    "    modelLogits.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    modelLogits.add(BatchNormalization())\n",
    "    modelLogits.add(Dropout(0.5)) \n",
    "\n",
    "    modelLogits.add(Conv2D(256, (5, 5) ) )   \n",
    "    modelLogits.add(LeakyReLU(alpha=0.01))\n",
    "    modelLogits.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    modelLogits.add(BatchNormalization())\n",
    "    modelLogits.add(Dropout(0.5)) \n",
    "\n",
    "    modelLogits.add(Flatten())\n",
    "    modelLogits.add(LeakyReLU(alpha=0.0)) \n",
    "    modelLogits.add(Dense(384))\n",
    "    modelLogits.add(LeakyReLU(alpha=0.0))             \n",
    "    modelLogits.add(Dropout(0.5)) \n",
    "\n",
    "    modelLogits.add(Dense(classCount))\n",
    "\n",
    "    output = Activation('softmax')(modelLogits.output)\n",
    "\n",
    "    model = tf.keras.Model(modelLogits.inputs, output) \n",
    "\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "    model.compile(optimizer = opt, loss='categorical_crossentropy', metrics=[ 'accuracy'])\n",
    "    return model, modelLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS = len(trained_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train, val):\n",
    "\n",
    "    models = []\n",
    "    histories = []\n",
    "    \n",
    "    for i in range(NUM_MODELS):\n",
    "\n",
    "        model, modelL = create_model(NUM_CLASSES,IMAGE_SIZE,3)\n",
    "\n",
    "        if TRAIN_ONLINE:\n",
    "            callbacks = prepare_callbacks(train_models[i])\n",
    "\n",
    "            hist = model.fit(train, \n",
    "                            epochs=100, \n",
    "                            validation_data = val, \n",
    "                            callbacks = callbacks)\n",
    "\n",
    "        models.append([model, modelL])\n",
    "        histories.append(hist)\n",
    "    \n",
    "    return models,histories\n",
    " \n",
    "\n",
    "\n",
    "models_V1, histories_V1 = train_models(dataSolo, valsetx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(models, file_path_prefix):\n",
    "\n",
    "    for i in range(NUM_MODELS):\n",
    "\n",
    "        file_path = f'{file_path_prefix}_{i:02}/cp.ckpt'\n",
    "\n",
    "        models[i][0].load_weights(file_path)\n",
    "        models[i][0].save('temp.hdf5')\n",
    "        models[i][1].load_weights('temp.hdf5', by_name=True)\n",
    "    \n",
    "\n",
    "load_weights(models_V1, file_path_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models):\n",
    "    \n",
    "    accuracy = 0\n",
    "\n",
    "    for i in range(NUM_MODELS):\n",
    "\n",
    "        eval = models[i][0].evaluate(testset, verbose = 2)\n",
    "        accuracy += eval[1]\n",
    "\n",
    "    print(f'average accuracy: {(accuracy/NUM_MODELS)*100:.3f}')    \n",
    "\n",
    "\n",
    "evaluate_models(models_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_logits_and_preds(models):\n",
    "\n",
    "    preds = [[] for _ in range(NUM_MODELS) ]\n",
    "    logits = [[] for _ in range(NUM_MODELS)]\n",
    "    labels = []\n",
    "    for images, labs in testset.take(-1):\n",
    "\n",
    "        labels.extend(labs.numpy())\n",
    "        for i in range(NUM_MODELS):\n",
    "\n",
    "            preds[i].extend(models[i][0].predict(images))\n",
    "            logits[i].extend(models[i][1].predict(images))\n",
    "\n",
    "    labels = [np.argmax(i) for i in labels]  \n",
    "    \n",
    "    return labels, logits, preds\n",
    "\n",
    "labels_V1, logits_V1, preds_V1 = get_labels_logits_and_preds(models_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_preds(preds):\n",
    "\n",
    "    class_preds = []\n",
    "\n",
    "    for i in range(test_dataset_length):\n",
    "\n",
    "        c = []\n",
    "        for m in range(NUM_MODELS):\n",
    "\n",
    "            c.append(np.argmax(preds[m][i]))\n",
    "        class_preds.append(c)\n",
    "        \n",
    "    return class_preds\n",
    "\n",
    "\n",
    "class_preds_V1 = get_class_preds(preds_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_from_sum_of_logits(logits):\n",
    "\n",
    "    sum_logits = []\n",
    "\n",
    "    for i in range(test_dataset_length):\n",
    "\n",
    "        log = logits[0][i]\n",
    "        for m in range(1, NUM_MODELS):\n",
    "            log = np.add(log, logits[m][i])\n",
    "        sum_logits.append(np.argmax(log))\n",
    "    return(sum_logits)\n",
    "    \n",
    "class_logits_V1 = get_class_from_sum_of_logits(logits_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(labels, class_preds, class_logits):\n",
    "\n",
    "    all_correct = 0\n",
    "    all_incorrect = 0\n",
    "    maj_vote = 0\n",
    "    maj_wrong = 0\n",
    "    tie = 0\n",
    "    count = 0\n",
    "    log_ok = 0\n",
    "    log_ko = 0\n",
    "\n",
    "    for k in range(test_dataset_length):\n",
    "\n",
    "        counter = collections.Counter(class_preds[k])\n",
    "        if len(counter) == 1:\n",
    "            if counter.most_common(1)[0][0] == labels[k]:\n",
    "                all_correct += 1\n",
    "            else:\n",
    "                all_incorrect += 1\n",
    "        else:\n",
    "            aux = counter.most_common(2)\n",
    "            if aux[0][1] > aux[1][1] and aux[0][0] == labels[k]:\n",
    "                maj_vote += 1\n",
    "            if aux[0][1] > aux[1][1] and aux[0][0] != labels[k]:\n",
    "                maj_wrong += 1\n",
    "            elif aux[0][1] == aux[1][1]:\n",
    "                tie += 1\n",
    "        if class_logits[k] == labels[k]:\n",
    "            log_ok += 1\n",
    "        else:\n",
    "            log_ko += 1\n",
    "        count += 1 \n",
    "        \n",
    "    return [count, all_correct, all_incorrect, maj_vote, tie, maj_wrong, log_ok, log_ko]\n",
    "    \n",
    "    \n",
    "res = get_stats(labels_V1, class_preds_V1, class_logits_V1)\n",
    "print(res, res[6]/res[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
