{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, LeakyReLU \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "\n",
    "# to display confusion matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == classNames\n",
    "\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_png(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [WIDTH, HEIGHT])\n",
    "\n",
    "\n",
    "def get_bytes_and_label(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_accuracies(labels, test): \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    X = np.arange(len(test))\n",
    "\n",
    "    plt.bar(X, test, width = 0.4, color = 'b', label='test')\")\n",
    "    plt.xticks(X + 0.4 / 2, labels)\n",
    "    plt.ylim(top = 1.0, bottom = 0.97)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathTrain = 'GTSRB_TP/train_images'\n",
    "#pathTrainBalance = 'GTSRB_TP/train_images'\n",
    "pathTest = 'GTSRB_TP/test_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "HEIGHT = 32\n",
    "WIDTH = 32\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "#Classes names\n",
    "data_dir = pathlib.Path(pathTrain)\n",
    "classNames = np.array(os.listdir(data_dir))\n",
    "print(classNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = ['TrainModels/static_Augmentation.ckpt', 'TrainModels/static_Augmentation_balance.ckpt',\n",
    "'TrainModels/noAugmentation.ckpt','TrainModels/noAugmentation_Balance.ckpt', 'TrainModels/Dynamic_Augmentation.ckpt', 'Massive_Augmentation.ckpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listset = tf.data.Dataset.list_files(pathTest+\"/*/*.png\")\n",
    "test_Data = listset.map(get_bytes_and_label, num_parallel_calls = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Data = test_Data.batch(batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(128, (5, 5),\n",
    "                    input_shape=(32, 32, 3)))         \n",
    "model.add(LeakyReLU(alpha=0.01))  \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5)) \n",
    "\n",
    "model.add(Conv2D(196, (5, 5) )) \n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5)) \n",
    "\n",
    "model.add(Conv2D(256, (5, 5) ) )   \n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5)) \n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(LeakyReLU(alpha=0.0)) \n",
    "model.add(Dense(384))\n",
    "model.add(LeakyReLU(alpha=0.0))             \n",
    "model.add(Dropout(0.5)) \n",
    "\n",
    "model.add(Dense(43, activation='softmax'))\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(optimizer = opt, loss='categorical_crossentropy', metrics=[ 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsloaded = []\n",
    "for i in range(len(trained_models)):\n",
    "    model.load_weights(trained_models[i])\n",
    "    modelsloaded[i] = model.evaluate(test_Data, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_accuracies(trained_models, modelsloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
